{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Methods - Track 1\n",
    "### 1. Policy Gradient for a Mixture of Policies\n",
    "Define $$v(s,\\theta,\\omega) = \\sum_{o} \\mu(o|s,\\theta) \\sum_a \\pi(a|s,o,\\omega) \\Bigl( r(s,a)+\\gamma\\sum_s'P(s'|s,a)v(s',\\theta,\\omega) \\Bigr)    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial v(s,\\theta,\\omega)}{\\partial \\omega_i} = \\sum_{o} \\mu(o|s,\\theta) \\frac{\\partial}{\\partial \\omega_i} \\sum_a \\pi(a|s,o,\\omega) \\Bigl( r(s,a)+\\gamma\\sum_s'P(s'|s,a)v(s',\\theta,\\omega) \\Bigr) \\\\\n",
    "= \\sum_{o} \\mu(o|s,\\theta) \\frac{1}{1-\\gamma}\\mathbf{E} \\left[\\sum_a q(s,a) \\frac{\\partial \\pi(a|s,o,\\omega)}{\\partial \\omega_i}  \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Last line is obtained from policy gradient theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial v(s,\\theta,\\omega)}{\\partial \\theta_i} = \\\\\n",
    "\\sum_o \\frac{\\partial \\mu(o,s,\\theta)}{\\partial \\theta_i} \\sum_a \\pi(a|s,o,\\omega)\\left(r(s,a) + \\gamma\\sum_{s'}P(s'|s,a)v(s',\\theta,\\omega)\\right) + \\\\\n",
    "\\sum_o \\mu(o|s,\\theta) \\sum_a \\pi(a|s,o,\\omega) \\gamma\\sum_{s'}P(s'|s,a)\\frac{\\partial v(s',\\theta,\\omega)}{\\partial\\theta_i}\\tag{1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $g_{\\theta_i} = \\frac{\\partial v(s,\\theta,\\omega)}{\\partial \\theta_i} $\n",
    "Let $h_{\\theta_i} = \\sum_o \\frac{\\partial \\mu(o,s,\\theta)}{\\partial \\theta_i} \\sum_a \\pi(a|s,o,\\omega)\\left(r(s,a) + \\gamma\\sum_{s'}P(s'|s,a)v(s',\\theta,\\omega)\\right)$\n",
    "\n",
    "Both $g_{\\theta_i}$ and $h_{\\theta_i} $ are $R^{|S|*1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $P(s,s') = \\sum_o \\mu(o|s,\\theta) \\sum_a \\pi(a|s,o,\\omega) P(s'|s,a)$, where $P \\in R^{|S|*|S|}$\n",
    "\n",
    "Now we can write Equation (1) as\n",
    "\\begin{equation*}\n",
    "g_{\\theta_i} =  h_{\\theta_i} + \\gamma Pg_{\\theta_i} \\\\\n",
    "\\equiv g_{\\theta_i} = (I-\\gamma P)^{-1}h_{\\theta_i}\n",
    "\\end{equation*}\n",
    "\n",
    "Since $\\gamma$ is the discount factor and is less than 1, $\\sigma(\\gamma P)<1$ and $(I-\\gamma P)^{-1}$ exists. Moreover, $(I-\\gamma P)^{-1} = \\sum_{t=0}^{\\infty}\\gamma^tP$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial v(s,\\theta,\\omega)}{\\partial \\theta_i} = g_{\\theta_i} = (I-\\gamma P)^{-1}h_{\\theta_i} = \\sum_{t=0}^{\\infty}\\gamma^tPh_{\\theta_i} = \\frac{1}{1-\\gamma} \\mathbf{E} [h_{\\theta_i}] \\\\\n",
    "= \\frac{1}{1-\\gamma} \\mathbf{E} \\left[ \\sum_o \\frac{\\partial \\mu(o,s,\\theta)}{\\partial \\theta_i} \\sum_a \\pi(a|s,o,\\omega)\\left(r(s,a) + \\gamma\\sum_{s'}P(s'|s,a)v(s',\\theta,\\omega)\\right) \\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Gradient Hessian\n",
    "Derive a policy Hessian theorem for the discounted case. You can follow the same derivation as for the first order policy gradient theorem shown in class. \n",
    "First order policy gradient theorm:\n",
    "$$\\frac{\\partial V_\\theta(S_0)}{\\partial \\theta_i}=\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i}Q_\\theta(s,a)=\\frac{1}{1-\\gamma}\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i}Q_\\theta(s,a)\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that, we can derive the Hessian matrix for policy gradient:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial V_\\theta(S_0)}{\\partial \\theta_i \\partial \\theta_j} & = \\frac{1}{1-\\gamma}\\frac{\\partial}{\\partial\\theta_j}\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i}Q_\\theta(s,a) \\\\\n",
    " & = \\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)+\\frac{1}{1-\\gamma}\\sum\\limits_a\\nabla_{\\theta_i}\\pi(a|s)\\frac{\\partial Q_\\theta(s,a)}{\\partial \\theta_j}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial Q_\\theta(s,a)}{\\partial\\theta_j}&=\\frac{\\partial}{\\partial\\theta_j}\\big[r(s,a)+\\gamma\\sum\\limits_{s'}P(s'|s,a)V_\\theta(s')\\big] \\\\\n",
    "&=\\gamma\\sum\\limits_{s'}P(s'|s,a)\\frac{\\partial}{\\partial \\theta_j}V_\\theta(s') \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Rightarrow\\frac{\\partial V_\\theta(S_0)}{\\partial\\theta_i\\partial\\theta_j}&=\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)+\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i}\\gamma\\sum\\limits_{s'}P(s'|s,a)\\frac{\\partial}{\\partial\\theta_j}V_\\theta(s')\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "From policy gradient theorm we know that $$\\frac{\\partial V_\\theta(S_0)}{\\partial\\theta_i\\theta_j}\\doteq g_\\theta(s)=(I-\\gamma P_\\theta)^{-1}h_\\theta$$\n",
    "where, $$P_\\theta\\doteq\\sum\\limits_a\\pi_\\theta(a|s)P(s'|s,a)$$\n",
    "$$h_\\theta\\doteq\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $$g_\\theta(s)=\\sum\\limits_{t=0}^\\infty\\gamma^t Ph_{\\theta_i}=\\frac{1}{1-\\gamma}\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_j}Q_\\theta(s,a)\\big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Rightarrow\\frac{\\partial V_\\theta(S_0)}{\\partial\\theta_i\\partial\\theta_j}&=\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)+\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i}\\gamma\\sum\\limits_{s'}P(s'|s,a)\\frac{1}{1-\\gamma}\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_j}Q_\\theta(s,a)\\big]\\\\\n",
    "&=\\frac{1}{1-\\gamma}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)+\\frac{1}{1-\\gamma}\\gamma\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_j}\\big]\\frac{1}{1-\\gamma}\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_j}Q_\\theta(s,a)\\big]\\\\\n",
    "&=\\frac{1}{1-\\gamma}\\mathbb{E}\\big[\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i\\partial\\theta_j}Q_\\theta(s,a)+\\frac{\\gamma}{1-\\gamma}\\sum\\limits_a\\frac{\\partial\\pi_\\theta(a|s)}{\\partial\\theta_i}\\sum\\limits_a\\frac{\\partial \\pi_\\theta(a|s)}{\\partial\\theta_j}Q_\\theta(s,a)\\big]\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constrained Optimization / Intrinsic Rewards\n",
    "$$ J_\\alpha(\\theta) = \\mathbf{E}_{s_0\\sim\\alpha, \\theta} \\left[\\sum_{t=0}^{\\infty}\\gamma^t r(S_t,A_t)\\right] - \\eta\\mathbf{E}_{s_0\\sim\\alpha, \\theta} \\left[\\sum_{t=0}^{\\infty}\\gamma^t c(S_t,A_t)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "J_\\alpha(\\theta) \n",
    "= \\mathbf{E}_{s_0\\sim\\alpha,\\theta} \\left[\\sum_{t=0}^{\\infty} \\gamma^t \\left(r(S_t,A_t)-\\eta c(S_t,A_t)\\right) \\right] \\\\\n",
    " = \\mathbf{E}_{s_0\\sim\\alpha,\\theta} \\left[ r(S_0,A_0) - \\eta c(S_0,A_0) + \\sum_{t=1}^{\\infty}\\gamma^t \\left(r(S_t,A_t)-\\eta c(S_t,A_t)\\right)\\right] \\\\ \n",
    "  = \\mathbf{E}_{s_0\\sim\\alpha,\\theta} \\left[ r(S_0,A_0) - \\eta c(S_0,A_0) + \\gamma\\sum_{t=0}^{\\infty}\\gamma^t \\left[r(S_{t+1},A_{t+1})-\\eta c(S_{t+1},A_{t+1})\\right]\\right] \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is to maximize the reward gained by starting from $S_0$, we can write $J_\\alpha(\\theta)$ as \n",
    "\\begin{equation*}\n",
    "J_\\alpha(\\theta) = V(S_0) = \\mathbf{E}_{s_0\\sim\\alpha,\\theta} \\left[ r(S_0,A_0) - \\eta c(S_0,A_0) + \\gamma V(S_1)\\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the gradient of $V(S_0)$ with respect to $\\theta_i$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial V(S_0)}{\\partial \\theta_i} = \\mathbf{E}_{s_0\\sim\\alpha,\\theta} \\left[\\sum_{A_0} \\frac{\\partial \\pi(A_0|S_0,\\theta))}{\\partial \\theta_i} [r(S_0,A_0)-\\eta c(S_0,A_0)+\\gamma \\sum_{S'}P(S'|S_0,A_0)V_{\\pi}(S')] + \\gamma\\frac{\\partial V(S_1)}{\\partial \\theta_i}\\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last term in the above equation is $\\gamma\\frac{\\partial V(S_1)}{\\partial \\theta_i}$. By recursing on it, we obtain \n",
    "\\begin{equation*}\n",
    "\\frac{\\partial V(S_0)}{\\partial \\theta_i} = \\sum_{t=0}^{\\infty} \\gamma^t \\mathbf{E}_{s_t,\\theta}\\left[\\sum_{A_t} \\frac{\\partial \\pi(A_t|S_t,\\theta))}{\\partial \\theta_i} [r(S_t,A_t)-\\eta c(S_t,A_t)+\\gamma \\sum_{S'}P(S'|S_t,A_t)V_{\\pi}(S')] \\right]\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
