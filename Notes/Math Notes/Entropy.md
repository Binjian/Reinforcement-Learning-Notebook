## Entropy

- Named after [Boltzmann's Η-theorem](https://en.wikipedia.org/wiki/H-theorem), Shannon defined the entropy Η (Greek capital letter [eta](https://en.wikipedia.org/wiki/Eta)) of a discrete random variable *X* with possible values {*x*1, ..., *x**n*} and [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) P(*X*) as:

<img src="https://ws4.sinaimg.cn/large/006tNc79gy1fppq7s40k5j30du01qgln.jpg" width="400px"/>

  Here E is the [expected value operator](https://en.wikipedia.org/wiki/Expected_value), and I is the information content of *X*.[[4\]](https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_note-4)[[5\]](https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_note-5) I(*X*) is 	itself a random variable.

- The entropy can explicitly be written as:

  <img src="https://ws1.sinaimg.cn/large/006tNc79gy1fppq8n06u0j30kc02waab.jpg" width="500px" />

  In the case of P(*x**i*) = 0 for some *i*, the value of the corresponding summand 0 log*b*(0) is taken to be 0, which is consistent with the limit:

  <img src="https://ws4.sinaimg.cn/large/006tNc79gy1fppqb7q5gmj307q028749.jpg" width="200px"/>

- The entropy of the **multivariate normal distribution** is:

  ![](https://ws4.sinaimg.cn/large/006tNc79gy1fppqcbvb9uj312806g0tw.jpg)

  where the bars denote the matrix determinant and *k* is the dimensionality of the vector space.

