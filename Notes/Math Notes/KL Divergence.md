## Kullback-Leibler divergence (KL Divergence)

- **Kullback–Leibler divergence** (also called **relative entropy**) is a measure of how one probability distribution diverges from a second, expected probability distribution.

- Definition:

  ![](https://ws1.sinaimg.cn/large/006tNc79gy1fpppz0tltjj316y0aotan.jpg)

  In order words, it is the expectation of the logarithmic difference between the probabilities P and Q, where the expectation is taken using the probabilities P.

- Important Example - **Multivariate normal distributions**

  ![](https://ws3.sinaimg.cn/large/006tNc79gy1fppq0m3hxnj31360603zz.jpg)

  ​