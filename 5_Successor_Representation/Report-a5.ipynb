{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 - GVFs and Successor Representation\n",
    "### Jianing Sun 260791202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Common Question\n",
    "Explain how SR can be expressed in the GVF framework (also see corresponding section in S&B if necessary). Explain this connection using the GVF terminology : cumulant (this is not the notion of “moments” of a prob. distribution), termination condition, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In General Value Functions (GVF), whatever signal is added up in a value-function-like prediction, we call it the *cumulant* of that prediction. Denote $C_t\\in\\mathbb{R}$ as the **cumulant** signal, and **terminal function** $\\gamma: \\mathcal{S}\\mapsto[0,1]$. Then by definition, a **general value function**, or GCF, is written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi,\\gamma,C(s)}=\\mathbb{E}\\big[\\sum\\limits_{k=t}^{\\infty}C_{k+1}\\prod\\limits_{i=t+1}^{k}\\gamma(S_i)\\big|S_t=s,A_{t:\\infty}\\sim\\pi\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different choices of cumulant C(s) lead to different characterizations of the Markov Chian. For example, if C(s) specfies the reward received on entering state s, then $v_{\\pi,\\gamma,C(s)}$ correponds to the expected discounted future return, or value, the standard target of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of successor representation (SR), successor representation can be expressed as part of the GVF framework as this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi,\\gamma,C(s)}=\\sum\\limits_{s'\\in\\mathcal{S}}\\phi_{\\pi}(s,s')C(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\phi_\\pi$ is the successor representation (SR), which **encodes the expected discounted future visitations of each state s' along trajectories originating in state s.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Trace 1\n",
    "#### 1. Starting from the definition of above, derive Bellman equations for phi_pi. Note : phi_\\pi(s) has essentially the meaning of a “value function”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{split} \n",
    "\\phi_\\pi(s)=e_s^T(I-\\gamma P_\\pi)^{-1}=e_s^T\\sum\\limits_{t=0}^\\infty\\gamma^tP_\\pi \n",
    "=\\mathbb{E}\\big[e_s^T\\sum\\limits_{t=0}^\\infty\\gamma^t\\big]\n",
    "\\end{split} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{split} \n",
    "\\Rightarrow \\phi_\\pi(s)&=\\mathbb{E}\\big[\\sum\\limits_{t=0}^{\\infty}\\gamma^t\\mathbb{1}[S_{t+1}=s']\\big|S_o=s\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}[S_1=s']+\\sum\\limits_{t=1}^\\infty\\gamma^t\\mathbb{1}[S_{t+1}=s']\\big|S_0=s\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}[S_1=s']+\\sum\\limits_{t=0}^\\infty\\gamma^{t+1}\\mathbb{1}[S_{1+t+1}=s']\\big|S_0=s\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}[S_1=s']+\\gamma\\sum\\limits_{t=1}^\\infty\\gamma^{t}\\mathbb{1}[S_{1+t+1}=s']\\big|S_0=s\\big] \\\\\n",
    "&=\\mathbb{E}\\big[\\mathbb{1}[S_1=s']+\\gamma\\phi_\\pi(S1)\\big|S_0=s\\big] \\\\\n",
    "\\end{split} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbb{1}[\\cdot=\\cdot]$ is an indicator whose value is 1 if its argument is true and 0 otherwise. In other reference [paper](https://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map.pdf), they usually denote successor representation (SR) with two parameters as M(s,s'), where s' indicates all the other states $\\in$ S along the trajectory originating at state s. Therefore in our assignment, $\\phi_\\pi(s)\\equiv\\phi_\\pi(s,s')$, that means s' is given thus we do not need to marginalize over s' for next step derivation of the successor representation's Bellman Equation. We only need to marginalize over S1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{split}\n",
    "\\phi_\\pi(s)&=\\sum\\limits_{s1}T_{s,s_1}\\big[\\mathbb{1}[s_1=s']+\\gamma\\phi_\\pi(S1)\\big] \\\\\n",
    "&=\\sum\\limits_aP(s_1|s,a)\\sum\\limits_a\\pi(a|s)\\big[\\mathbb{1}[s_1=s']+\\gamma\\phi_\\pi(s_1)\\big] \\\\\n",
    "&=\\sum\\limits_a\\pi(a|s)\\big[\\sum\\limits_{s_1}P(s_1|s,a)\\mathbb{1}[s_1=s']+\\gamma\\sum\\limits_{s_1}P(s_1|s,a)\\phi_\\pi(s_1)\\big]\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
