{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mandatory Questions\n",
    "### 1.1 Bellman Optimality Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a contraction argument, show that there exists a solution to the Bellman optimality equations. That is : show that the Bellman optimality operator is a contraction mapping. (nonlinear case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Operators: <br /> define $T^{\\pi}: \\mathbb{R}^{\\mathcal{S}\\times{\\mathcal{A}}}$ and $T^{*}: \\mathbb{R}^{\\mathcal{S}\\times{\\mathcal{A}}}$ as follows: <br />\n",
    "$T^{\\pi}Q(s,a) = r(s,a)+\\gamma \\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a)V_{\\pi}(s^{'}) \\hspace{1cm} (s,a)\\in \\mathcal{S}\\times{\\mathcal{A}}$ <br \\>\n",
    "$T^{*}Q(s,a) = \\max_{a}\\big|r(s,a)+\\gamma \\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a)V_{\\pi}(s^{'})\\big| \\hspace{1cm} (s,a)\\in \\mathcal{S}\\times{\\mathcal{A}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: Consider the following statement<br />\n",
    "$$|\\max_{z}f(z)-\\max_{z}h(z)|\\leq \\max_{z}|f(z)-h(z)|$$\n",
    "Applying this to $TV(s)$ we have:<br />\n",
    "$TV_{k+1}(s)-TV_{*}(s) =\\max_{a}\\big|r(s,a)+\\gamma\\sum_{s^{'}}\\mathcal{p}(s^{'}|s,a)V_k(s^{'})\\big|-\\max_{a}\\big|r(s,a)+\\gamma\\sum_{s^{'}}\\mathcal{p}(s^{'}|s,a)V_{*}(s^{'})\\big| $ <br />\n",
    "$\\hspace{3.4cm} \\leq \\max_{a}\\big|\\gamma\\sum_{s^{'}}\\mathcal{p}(s^{'}|s,a)V_k(s^{'})-\\gamma\\sum_{s^{'}}\\mathcal{p}(s^{'}|s,a)V_k(s^{*})\\big|$<br />\n",
    "$\\hspace{3.4cm}\\leq \\max_{a}\\gamma\\big|\\sum_{s^{'}}\\mathcal{p}(s^{'}|s,a)(V_k(s^{'})-V_*(s^{'})\\big|$<br />\n",
    "$\\hspace{3.4cm}\\leq \\gamma\\max_{a}\\big|V_k(s^{'}-V_*(s^{'})\\big|$<br />\n",
    "$\\hspace{3.4cm}=\\gamma \\big|\\big|V_k(s)-V_*(s^{'})\\big|\\big|_\\infty$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, $TV_{k+1}(s)-TV_{*}(s) \\leq \\gamma^{k}||V_0(s)-V_*(s^{'})||$, giving the geometric convergence as $0< \\gamma <1$ <br />\n",
    "Now we show that the nonlinear Bellman operator T is contraction mapping with respect to the maximum num."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the values of two successive policies generated by policy iteration are nondecreasing. Assume a finite MDP and conclude (explain why) that policy iteration must terminate under a finite number of steps. Finally, show that upon termination, policy iteration must have found an optimal policy (ie. one which satisfies the optimality equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Monotonicity of Bellman Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prove $V(s)\\leq V^{'}(s),\\forall s\\in \\mathcal{S} \\Rightarrow TV(s) \\leq TV^{'}(s), \\forall s \\in \\mathcal{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: Insert into $TV(s)$,<br />\n",
    "$TV(s) = \\max_{a}\\big|r(s,a)+\\gamma\\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a)V(s^{'})\\big|$ <br />\n",
    "$\\hspace{1.17cm}= r(s,a_{V}^{*})+\\gamma \\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a_{V}^{*})V(s^{'})$ <br />\n",
    "$\\hspace{1.17cm}\\leq r(s,a_{V^{'}}^{*})+\\gamma \\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a_{V^{'}}^{*})V(s^{'})$ <br />\n",
    "$\\hspace{1.17cm}\\leq r(s,a_{V^{'}}^{*})+\\gamma \\sum_{s^{'}}\\mathcal{P}(s^{'}|s,a_{V^{'}}^{*})V^{'}(s^{'})$<br />\n",
    "$\\hspace{1.17cm} = TV^{'}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show that if $V(s)\\leq V^{'}(s),\\forall s\\in \\mathcal{S}$ then $TV(s) \\leq TV^{'}(s), \\forall s \\in \\mathcal{S}$. Which means the values of twoo successive policies generated by policy iteration are nondecreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Reason of termination after a finite number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when state and policy are finite, the problem becomes a deterministic shortest path problem with nonnegative arc lengths. If all cycles of the state transition graph have positive length, all policies $\\pi$ that do not terminate from a state $s \\in \\mathcal{S}$ must satisfy $V_{\\pi}(s) = \\infty$, implying that there exists a final policy that terminates from all $s \\in \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Proof of optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: denote $V_{t}^{*} = T_{t}(V_{t+1}^{*})$, $V_{t}^{\\pi} \\leq T_{t}(V_{t+1}^{\\pi})$, and $V_{T}^{*}=V_{T}^{\\pi}$ <br />\n",
    "$\\hspace{2.3cm}V_{t}^{\\pi} \\leq T_{t}(V_{t+1}^{\\pi})$ <br />\n",
    "$\\hspace{3.0cm}\\leq T_{t}T_{t+1}(V_{t+2}^{\\pi})$ <br />\n",
    "$\\hspace{3.0cm}$.<br />\n",
    "$\\hspace{3.0cm}$.<br />\n",
    "$\\hspace{3.0cm}$.<br />\n",
    "$\\hspace{3.0cm}\\leq T_{t}T_{t+1}...T_{T-1}(V_{T}^{\\pi})$ <br />\n",
    "$\\hspace{3.0cm}=T_{t}T_{t+1}...T_{T-1}(V_{T}^{*})$ <br />\n",
    "$\\hspace{3.0cm}=V_{t}^{*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, any policy defined by dynamic programming is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Track 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and compare empirically the performance of value iteration, policy iteration and modified policy iteration. Modified policy iteration is a simple variant of policy iteration in which the evaluation step is only partial.<br />\n",
    "**Transition Probabilities:** <br />\n",
    "P(s_0 | s_0, a_0) = 0.5, P(s_1 | s_0, a_0) = 0.5, P(s_0 | s_0, a_1) = 0, P(s_1 | s_0, a_1) = 1, P(s_1 | s_0, a_2) = 0, P(s_1 | s_1, a_2) = 1 <br />\n",
    "**Rewards:** r(s_0, a_0) = 5, r(s_0, a_1) = 10, r(s_1, a_2) = -1 <br />\n",
    "**discount factor:** 0.95 <br />\n",
    "Also run your experiments in a second MDP of your choice (chain MDP, grid world, etc.). Explain and explore how the convergence rate of these algorithms is affected by the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the 2-state MDP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "states = [0, 1]\n",
    "# actions = [[0, 1], [2]]\n",
    "actions = [0, 1, 2]\n",
    "discount_factor = 0.95\n",
    "\n",
    "# Transition Probabilities: P[s',s,a]  Rewards: r[s,a]\n",
    "P = np.zeros([len(states), len(states), len(actions)])\n",
    "R = np.zeros([len(states), len(actions)])\n",
    "\n",
    "P[0, 0, 0] = 0.5\n",
    "P[1, 0, 0] = 0.5\n",
    "P[0, 0, 1] = 0\n",
    "P[1, 0, 1] = 1\n",
    "P[1, 0, 2] = 0\n",
    "P[1, 1, 2] = 1\n",
    "\n",
    "R[0, 0] = 5\n",
    "R[0, 1] = 10\n",
    "R[1, 2] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Algorithm 1: Policy Iteration\n",
    "### First Step: Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, P, R, gamma, theta=0.0001):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        policy: |state|x|action| matrix. This is a deterministic policy for this 2-state markov decision process.\n",
    "                Hence for every iteration there is a probability 1 in each row of this policy matrix.\n",
    "        P: transition probability matrix. |next_state|x|state|x|action|\n",
    "        R: reward matrix. |state|x|action|\n",
    "        gamma: discount factor equals 0.95\n",
    "        theta: repeat until delta < theta, then break\n",
    "    Return:\n",
    "        return value function with length of states\n",
    "    \"\"\"\n",
    "    # Initialize Value function with all 0\n",
    "    V = np.zeros(len(states))\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # for each state, conduct Bellman backup\n",
    "        for s in range(len(states)):\n",
    "            v = 0\n",
    "            for a, a_prob in enumerate(policy[s]):\n",
    "                # each state has a valid action set, here we mask out unavailbale actions.\n",
    "                if (s == 0 and a == 2) or (s == 1 and a == 0) or (s == 1 and a == 1):\n",
    "                    continue\n",
    "                # calculate value for each state\n",
    "                v += sum([a_prob * P[next_s, s, a] * (R[s, a] + gamma * V[next_s]) for next_s in range(len(states))])\n",
    "            # terminal condition (calculate the difference of 2 successive iterations)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        counter += 1\n",
    "        # terminal check\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Step: Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improve(policy_eval, gamma=0.95):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy_eval: policy evaluation function with value function returned. After every evaluation we can get the \n",
    "                     value function, then improve current policy based on value function.\n",
    "        gamma: discount factor equals 0.95.\n",
    "        \n",
    "    Returns:\n",
    "        return the optimal policy associated with the value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize policy matrix arbitrarily for all states\n",
    "    policy = np.zeros([len(states), len(actions)], dtype=int)\n",
    "    policy[0][0] = 1\n",
    "    policy[1][2] = 1\n",
    "\n",
    "    while True:\n",
    "        # obtain value function of current policy\n",
    "        V = policy_eval(policy, P, R, gamma=0.95)\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(len(states)):\n",
    "            \n",
    "            # find the old best action\n",
    "            old_a = np.argmax(policy[s])\n",
    "\n",
    "            # policy improvement by updating the best action\n",
    "            action_array = np.full(len(actions), fill_value=-np.inf)  # np.zeros(len(actions))\n",
    "            for a in range(len(actions)):\n",
    "                # mask out invalid situations\n",
    "                if (s == 0 and a == 2) or (s == 1 and a == 0) or (s == 1 and a == 1):\n",
    "                    continue\n",
    "                # calculate action probability for every action and find the one with probability 1\n",
    "                action_array[a] = sum(\n",
    "                    P[next_s, s, a] * (R[s, a] + gamma * V[next_s]) for next_s in\n",
    "                    range(len(states)))\n",
    "            #update best option\n",
    "            new_a = np.argmax(action_array)\n",
    "            # for every 2 successive iteration, check the difference\n",
    "            if new_a != old_a:\n",
    "                policy_stable = False\n",
    "            # save improved policy\n",
    "            policy[s] = np.eye(len(actions))[new_a]\n",
    "        # if our policy is stable, then stop iterating and return optimal policy with value function\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "[[1 0 0]\n",
      " [0 0 1]]\n",
      "\n",
      "Value Function:\n",
      "[ -8.56957076 -19.99814218]\n"
     ]
    }
   ],
   "source": [
    "policy, V = policy_improve(policy_eval)\n",
    "print(\"Policy:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2) Algorithm 2: Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, R, theta=0.0001, discount_factor=0.95):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: transition probability matrix. |next_state|x|state|x|action|\n",
    "        R: reward matrix. |state|x|action|\n",
    "        theta: 1e-4, a smaill positive threhold. Repeat until delta < theta then break\n",
    "    \n",
    "    Returns:\n",
    "        policy: final optimal policy matrix. |state|x|action|\n",
    "        V: value function\n",
    "    \"\"\"\n",
    "    # one sweep (one update of each state)\n",
    "    def one_sweep(state, V):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: current state s\n",
    "            V: last value function input for onew sweep updating\n",
    "        \n",
    "        Returns:\n",
    "            return the action value function used to find current best action \n",
    "        \"\"\"\n",
    "        action_val = np.zeros(len(actions))\n",
    "        # A = np.full(len(actions), fill_value=-np.inf)\n",
    "        for a in range(len(actions)):\n",
    "            action_val[a] = sum([P[next_s, state, a] * (R[state, a] + discount_factor * V[next_s]) for next_s in\n",
    "                        range(len(states))])\n",
    "            # mask out invalid situations\n",
    "            if (state == 0 and a == 2) or (state == 1 and a == 0) or (state == 1 and a == 1):\n",
    "                action_val[a] = -np.inf\n",
    "        return action_val\n",
    "    \n",
    "    # initialize value function with all 0\n",
    "    V = np.zeros(len(states))\n",
    "    counter = 0\n",
    "    \n",
    "    # repeat until delta < theta\n",
    "    while True:\n",
    "        counter += 1\n",
    "        delta = 0\n",
    "        for s in range(len(states)):\n",
    "            action_val = one_sweep(s, V)           \n",
    "            # check the difference between 2 successtive iteration\n",
    "            delta = max(delta, np.abs(np.max(action_val) - V[s]))\n",
    "            # save current best value function\n",
    "            V[s] = np.max(action_val)\n",
    "            \n",
    "        if delta < theta:\n",
    "            break\n",
    "    # initialize policy with all zero\n",
    "    policy = np.zeros([len(states), len(actions)])\n",
    "    \n",
    "    # output a deterministic policy\n",
    "    for s in range(len(states)):\n",
    "        \n",
    "        action_val = one_sweep(s, V)\n",
    "        # find the best action\n",
    "        best_action = np.argmax(action_val)\n",
    "        # set the policy \n",
    "        policy[s, best_action] = 1.0\n",
    "        \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: \n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Value Function:\n",
      "[ -8.56957076 -19.99814218]\n"
     ]
    }
   ],
   "source": [
    "policy, V = value_iteration(P, R)\n",
    "print(\"Policy: \")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3: Modified Policy iteration\n",
    "### First Step: Modified Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def modified_evaluation(pi, P, R, U, k=20):\n",
    "    for i in range(k):\n",
    "        for s in range(len(states)):\n",
    "            if (s == 0 and pi[s] == 2) or (s == 1 and pi[s] == 0) or (s == 1 and pi[s] == 1):\n",
    "                continue\n",
    "            # U[s] = R[s, pi[s]] + discount_factor * sum([P[next_s, s, pi[s]] * U[next_s] for next_s in range(len(states))])\n",
    "            U[s] = sum([P[next_s, s, pi[s]] * (R[s, pi[s]] + discount_factor * U[next_s]) for next_s in range(len(states))])\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Step: Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    U = dict([(s, 0) for s in states])\n",
    "    pi = dict([(s, random.choice(actions)) for s in states])\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        U = modified_evaluation(pi, P, R, U)\n",
    "        policy_stable = True\n",
    "        for s in range(len(states)):\n",
    "            action_array = np.full(len(actions), fill_value=-np.inf)  # np.zeros(len(actions))\n",
    "            for a in range(len(actions)):\n",
    "\n",
    "                if (s == 0 and a == 2) or (s == 1 and a == 0) or (s == 1 and a == 1):\n",
    "                    continue\n",
    "\n",
    "                action_array[a] = sum(\n",
    "                    P[next_s, s, a] * (R[s, a] + discount_factor * U[next_s]) for next_s in\n",
    "                    range(len(states)))\n",
    "\n",
    "            new_a = np.argmax(action_array)\n",
    "\n",
    "            if new_a != pi[s]:\n",
    "                pi[s] = new_a\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            return pi      #, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 2}\n"
     ]
    }
   ],
   "source": [
    "policy = policy_iteration()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting grid world MDP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "def gridWorld():\n",
    "\n",
    "    shape = [4, 4]\n",
    "    # define the boundary\n",
    "    MAX_X = shape[0]\n",
    "    MAX_Y = shape[1]\n",
    "\n",
    "    # define the grid\n",
    "    nActions = 4\n",
    "    nStates = np.prod(shape)\n",
    "\n",
    "    grid = np.arange(nStates).reshape([nActions, nActions])\n",
    "    P = {}\n",
    "\n",
    "    # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.nditer.html\n",
    "    it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "    while not it.finished:\n",
    "        s = it.iterindex\n",
    "        y, x = it.multi_index  # y -> number of row, x -> number of column\n",
    "\n",
    "        # initialize prob distribution\n",
    "        P[s] = {a: [] for a in range(nActions)}\n",
    "\n",
    "        # check if arrive at terminal states (position 0 and 15)\n",
    "        check_terminal = lambda s: s == 0 or s == (nStates - 1)\n",
    "        reward = 0.0 if check_terminal(s) else -1.0\n",
    "\n",
    "        if check_terminal(s):\n",
    "            # got terminal, next state should be itself\n",
    "            P[s][UP] = [(1.0, s, reward)]\n",
    "            P[s][DOWN] = [(1.0, s, reward)]\n",
    "            P[s][LEFT] = [(1.0, s, reward)]\n",
    "            P[s][RIGHT] = [(1.0, s, reward)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            ns_up = s if y == 0 else s - MAX_X\n",
    "            ns_right = s if x == (MAX_X - 1) else s + 1\n",
    "            ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
    "            ns_left = s if x == 0 else s - 1\n",
    "\n",
    "            P[s][UP] = [(1.0, ns_up, reward)]\n",
    "            P[s][RIGHT] = [(1.0, ns_right, reward)]\n",
    "            P[s][DOWN] = [(1.0, ns_down, reward)]\n",
    "            P[s][LEFT] = [(1.0, ns_left, reward)]\n",
    "\n",
    "        it.iternext()\n",
    "\n",
    "    # Initial state distribution is uniform\n",
    "    isd = np.ones(nStates) / nStates\n",
    "\n",
    "    return nStates, nActions, P, isd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, P, discount_factor=0.95, theta=1e-4):\n",
    "    V = np.zeros(nStates)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(nStates):\n",
    "            v = 0\n",
    "\n",
    "            for a, a_prob in enumerate(policy[s]):\n",
    "\n",
    "                for prob, next_s, reward in P[s][a]:\n",
    "                    v += a_prob * prob * (reward + discount_factor * V[next_s])\n",
    "\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_improvement(P, policy_eval_fn=policy_evaluation, discount_factor=0.95):\n",
    "    policy = np.ones([nStates, nActions]) / nActions\n",
    "\n",
    "    while True:\n",
    "\n",
    "        V = policy_eval_fn(policy, P, discount_factor)\n",
    "\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(nStates):\n",
    "\n",
    "            old_a = np.argmax(policy[s])\n",
    "\n",
    "            action_array = np.zeros(nActions)\n",
    "\n",
    "            for a in range(nActions):\n",
    "                for prob, next_s, reward in P[s][a]:\n",
    "                    action_array[a] += prob * (reward + discount_factor * V[next_s])\n",
    "\n",
    "            new_a = np.argmax(action_array)\n",
    "\n",
    "            if old_a != new_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(nActions)[new_a]\n",
    "\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "value\n",
      "[ 0.     -1.     -1.95   -2.8525 -1.     -1.95   -2.8525 -1.95   -1.95\n",
      " -2.8525 -1.95   -1.     -2.8525 -1.95   -1.      0.    ]\n",
      "\n",
      "[[ 0.     -1.     -1.95   -2.8525]\n",
      " [-1.     -1.95   -2.8525 -1.95  ]\n",
      " [-1.95   -2.8525 -1.95   -1.    ]\n",
      " [-2.8525 -1.95   -1.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "nStates, nActions, P, isd = gridWorld()\n",
    "\n",
    "policy, v = policy_improvement(P)\n",
    "\n",
    "print(\"policy\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"value\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(v.reshape([4, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(P, theta=0.0001, discount_factor=0.95):\n",
    "    def one_sweep(state, V):\n",
    "        action_val = np.zeros(nActions)\n",
    "        # A = np.full(len(actions), fill_value=-np.inf)\n",
    "        for a in range(nActions):\n",
    "            for prob, next_s, reward in P[s][a]:\n",
    "                action_val[a] = prob * (reward + discount_factor * V[next_s])\n",
    "            if (state == 0 and a == 2) or (state == 1 and a == 0) or (state == 1 and a == 1):\n",
    "                action_val[a] = -np.inf\n",
    "        return action_val\n",
    "\n",
    "    V = np.zeros(nStates)\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        counter += 1\n",
    "        delta = 0\n",
    "        for s in range(nStates):\n",
    "            action_val = one_sweep(s, V)\n",
    "            best_action_value = np.max(action_val)\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            V[s] = best_action_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros([nStates, nActions])\n",
    "    for s in range(nStates):\n",
    "        action_val = one_sweep(s, V)\n",
    "        best_action = np.argmax(action_val)\n",
    "\n",
    "        policy[s, best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "value\n",
      "[ 0.     -1.     -1.95   -2.8525 -1.     -1.95   -2.8525 -1.95   -1.95\n",
      " -2.8525 -1.95   -1.     -2.8525 -1.95   -1.      0.    ]\n",
      "\n",
      "[[ 0.     -1.     -1.95   -2.8525]\n",
      " [-1.     -1.95   -2.8525 -1.95  ]\n",
      " [-1.95   -2.8525 -1.95   -1.    ]\n",
      " [-2.8525 -1.95   -1.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "nStates, nActions, P, isd = gridWorld()\n",
    "\n",
    "policy, V = value_iteration(P)\n",
    "\n",
    "print(\"policy\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"value\")\n",
    "print(V)\n",
    "print(\"\")\n",
    "\n",
    "print(V.reshape([4, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration for grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modified_evaluation(policy, P, discount_factor=0.95, k=20):\n",
    "    V = np.zeros(nStates)\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        for s in range(nStates):\n",
    "            v = 0\n",
    "\n",
    "            for a, a_prob in enumerate(policy[s]):\n",
    "\n",
    "                for prob, next_s, reward in P[s][a]:\n",
    "                    v += a_prob * prob * (reward + discount_factor * V[next_s])\n",
    "\n",
    "            V[s] = v\n",
    "\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_improvement(P, policy_eval_fn=policy_evaluation, discount_factor=0.95):\n",
    "    policy = np.ones([nStates, nActions]) / nActions\n",
    "\n",
    "    while True:\n",
    "\n",
    "        V = policy_eval_fn(policy, P, discount_factor)\n",
    "\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(nStates):\n",
    "\n",
    "            old_a = np.argmax(policy[s])\n",
    "\n",
    "            action_array = np.zeros(nActions)\n",
    "\n",
    "            for a in range(nActions):\n",
    "                for prob, next_s, reward in P[s][a]:\n",
    "                    action_array[a] += prob * (reward + discount_factor * V[next_s])\n",
    "\n",
    "            new_a = np.argmax(action_array)\n",
    "\n",
    "            if old_a != new_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(nActions)[new_a]\n",
    "\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-398470951c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnStates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnActions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgridWorld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"policy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "nStates, nActions, P = gridWorld()\n",
    "\n",
    "policy = policy_improvement(P)\n",
    "\n",
    "print(\"policy\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"value\")\n",
    "print(V)\n",
    "print(\"\")\n",
    "\n",
    "print(V.reshape([4, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
