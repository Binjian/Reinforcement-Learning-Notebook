## Dynamic Programming
### Policy Iteration
Once a policy, $\pi$, has been improved using  to yield a better policy, ⇡0, we can then compute v⇡0 and
improve it again to yield an even better ⇡00. We can thus obtain a sequence of monotonically improving
policies and value functions:
